import requests
import socket

HTTP_Prefix = 'http://' #@Todo: use href also.
DOUBLE_QUOTE = '"'
FORWARD_SLASH = "/"


def host_matches(ip_list_known, ip_list_check):
	for ip_check in ip_list_check:
		if ip_check in ip_list_known:
			return True

	return False


def nslookup(host):
	ip_list = []
	ais = socket.getaddrinfo(host,0,0,0,0)
	for result in ais:
	  ip_list.append(result[-1][0])
	ip_list = list(set(ip_list))
	#print(ip_list)
	return ip_list

def append_if_not_exist(list, object):
	if object not in list:
		list.append(object)

	return list

def append_list_if_not_exist(list, list_to_append):
	for object in list_to_append:
		if object not in list:
			list.append(object)

	return list

def merge_form_dicts(higher, lower):
	for key in lower.keys():
		if key not in higher.keys():
			higher[key] = lower[key]

	return higher


def split_href(split_response, host, previous_level_links):
	split_response = split_response.split('href="') # (DOUBLE_QUOTE + HTTP_Prefix)
	for i in range(0, len(split_response)):
		if HTTP_Prefix in split_response[i]:
			split_response[i] = split_response[i].split(HTTP_Prefix)[1]


	links = []

	for segment in split_response[1:]:
		split_segment = segment.split(DOUBLE_QUOTE)[0]
		if split_segment[0] == "#":
			split_segment = host + FORWARD_SLASH + split_segment
		elif split_segment[0] == "/":
			split_segment = host + split_segment

		link = HTTP_Prefix + split_segment
		#print(link)
		if (previous_level_links is None or link not in previous_level_links) and (link not in links):
			links.append(link)
			#print("Adding " + link)

	return links

def split_http(split_response, host, previous_level_links):
	split_response = split_response.split(DOUBLE_QUOTE + HTTP_Prefix)
	#for i in range(0, len(split_response)):
	#	if HTTP_Prefix in split_response[i]:
	#		split_response[i] = split_response[i].split(HTTP_Prefix)[1]

	links = []

	for segment in split_response[1:]:
		split_segment = segment.split(DOUBLE_QUOTE)[0]

		link = HTTP_Prefix + split_segment
		#print(link)
		if (previous_level_links is None or link not in previous_level_links) and (link not in links):
			links.append(link)
			#print("Adding " + link)

	return links


def clean_links(links):
	new_links = []
	for link in links:
		if not ("mailto:" in link or "https:" in link or "favicon.ico" in link or ".zip" in link or ".xml" in link or ".png" in link):
			new_links.append(link)

	return new_links

def http_crawl(url, host, crawl_externally, previous_level_links = None):


	print("Requesting: " + url + ", Host: " + host)
	#Get address info of host
	host_ip_list = nslookup(host)

	#Request root, / and common CGI pages if root does not exist .. need outside helper to call
	#url = 'http://' + host + ':' + str(port)

	try:
		response = requests.get(url=url)  # using HTTP Requests for humans: http://docs.python-requests.org/en/latest/
		#print(response.text)
	except:
		print("Request failed. Service is not HTTP.")
		return None, None


	print("Recvd response.")

	split_response = response.text.replace(' ', '')

	contains_form = False
	if "<input" in split_response:
		print("Page contains forms!")
		contains_form = True	
	
	links_href = split_href(split_response, host, previous_level_links)
	links_http = split_http(split_response, host, previous_level_links)
	links = append_list_if_not_exist(links_href, links_http)
	links = clean_links(links)

	for link in links:
		print(link)

	#exit(0)


	form_dict = {}
	form_dict[url] = contains_form

	this_level_links = links.copy()
	if previous_level_links:
		this_level_links = this_level_links + previous_level_links.copy()

	this_level_links.append(url)
	print("Number of links: " + str(len(links)))


	valid_links = {}
	for link in links:
		#if link not in lower_level_links:
		link_host = link.split(HTTP_Prefix)[1].split(FORWARD_SLASH)[0]
		print(link_host)
		try:
			link_ip_list = nslookup(link_host)
			if host_matches(host_ip_list, link_ip_list) or crawl_externally:
				valid_links[link] = link_host
				#print("Adding " + link)
		except:
			print("Skipping: " + link)


	lower_level_links = []

	#form_dict = valid_links.copy()
	#for link in form_dict.keys():

	#Find all links
	for link in valid_links.keys():
		#print("Crawling to: " + link)
		collected_links, collected_form_dict = http_crawl(link, valid_links[link], crawl_externally, this_level_links)
		append_list_if_not_exist(lower_level_links, collected_links)
		append_list_if_not_exist(this_level_links, collected_links)#make sure we don't visit the same page twice.
		form_dict = merge_form_dicts(form_dict, collected_form_dict)
	

	#Find any input forms
	#Make a dict of link, form boolean


	#for each link in links
		#sublist, forms = http_crawl

	#merge lists, forms

	#return list_of_links, forms

	#return links
	append_list_if_not_exist(lower_level_links, list(valid_links.keys()))
	return lower_level_links, form_dict



"""
['http://192.168.1.115/?feed=rss2&#038;author=1', 'http://192.168.1.115/wp-comments-post.php', 'http://192.168.1.115/?feed=rss2&#038;p=1', 'http://192.168.1.115/?feed=rss2&#038;cat=1', 'http://192.168.1.115/?feed=rss2&#038;page_id=2', 'http://192.168.1.115/wp-admin/', 'http://192.168.1.115/wp-login.php?action=lostpassword', 'http://192.168.1.115/?author=1', 'http://192.168.1.115/?p=1', 'http://192.168.1.115/?feed=comments-rss2', 'http://192.168.1.115/?p=1#comment-1', 'http://192.168.1.115/?p=1#comments', 'http://192.168.1.115/?cat=1', 'http://192.168.1.115/xmlrpc.php?rsd', 'http://192.168.1.115/?page_id=2', 'http://192.168.1.115/', 'http://192.168.1.115/wp-content/themes/twentythirteen/js/html5.js', 'http://192.168.1.115/?feed=rss2', 'http://192.168.1.115/wp-login.php', 'http://192.168.1.115/xmlrpc.php', 'http://192.168.1.115/wp-includes/wlwmanifest.xml']
['http://192.168.1.115/?feed=rss2&#038;p=1', 'http://192.168.1.115/wp-comments-post.php', 'http://192.168.1.115/?feed=rss2&#038;page_id=2', 'http://192.168.1.115/?feed=rss2&#038;author=1', 'http://192.168.1.115/wp-login.php?action=lostpassword', 'http://192.168.1.115/wp-admin/', 'http://192.168.1.115/?feed=rss2&#038;cat=1', 'http://192.168.1.115/wp-includes/wlwmanifest.xml', 'http://192.168.1.115/?p=1#comment-1', 'http://192.168.1.115/?page_id=2', 'http://192.168.1.115/?feed=comments-rss2', 'http://192.168.1.115/xmlrpc.php?rsd', 'http://192.168.1.115/wp-content/themes/twentythirteen/js/html5.js', 'http://192.168.1.115/?author=1', 'http://192.168.1.115/', 'http://192.168.1.115/wp-login.php', 'http://192.168.1.115/?feed=rss2', 'http://192.168.1.115/?p=1', 'http://192.168.1.115/?p=1#comments', 'http://192.168.1.115/xmlrpc.php', 'http://192.168.1.115/?cat=1']
['http://192.168.1.115/?feed=rss2&#038;cat=1', 'http://192.168.1.115/?feed=rss2&#038;p=1', 'http://192.168.1.115/wp-comments-post.php', 'http://192.168.1.115/wp-login.php?action=lostpassword', 'http://192.168.1.115/wp-admin/', 'http://192.168.1.115/?feed=rss2&#038;page_id=2', 'http://192.168.1.115/?feed=rss2&#038;author=1', 'http://192.168.1.115/?cat=1', 'http://192.168.1.115/?p=1', 'http://192.168.1.115/?feed=rss2', 'http://192.168.1.115/?p=1#comment-1', 'http://192.168.1.115/xmlrpc.php?rsd', 'http://192.168.1.115/wp-login.php', 'http://192.168.1.115/?feed=comments-rss2', 'http://192.168.1.115/?page_id=2', 'http://192.168.1.115/wp-content/themes/twentythirteen/js/html5.js', 'http://192.168.1.115/', 'http://192.168.1.115/xmlrpc.php', 'http://192.168.1.115/?p=1#comments', 'http://192.168.1.115/wp-includes/wlwmanifest.xml', 'http://192.168.1.115/?author=1']

"""