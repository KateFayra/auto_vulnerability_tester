import requests
import socket

HTTP_Prefix = 'http://'
DOUBLE_QUOTE = '"'
FORWARD_SLASH = "/"


def host_matches(ip_list_known, ip_list_check):
	for ip_check in ip_list_check:
		if ip_check in ip_list_known:
			return True

	return False


def nslookup(host):
	ip_list = []
	ais = socket.getaddrinfo(host,0,0,0,0)
	for result in ais:
	  ip_list.append(result[-1][0])
	ip_list = list(set(ip_list))
	#print(ip_list)
	return ip_list

def http_crawl(url, crawl_externally, previous_level_links = None):

	#Get address info of host
	host_ip_list = nslookup(host)

	#Request root, / and common CGI pages
	#url = 'http://' + host + ':' + str(port)
	print("Requesting: " + url)

	try:
		response = requests.get(url=url)  # using HTTP Requests for humans: http://docs.python-requests.org/en/latest/
		#print(response.text)
	except:
		print("Request failed. Service is not HTTP.")
		return False

	split_response = response.text.split(DOUBLE_QUOTE + HTTP_Prefix)
	#print(split_response)

	links = []

	for segment in split_response[1:]:
		link = HTTP_Prefix + segment.split(DOUBLE_QUOTE)[0]
		#print(link)
		if previous_level_links is None or link not in previous_level_links:
			links.append(link)

	this_level_links = links.copy()
	this_level_links.append(url)

	#Find all links
	for link in links:
		link_host = link.split(HTTP_Prefix)[1].split(FORWARD_SLASH)[0]
		link_ip_list = nslookup(link_host)
		print(link_host)
		if host_matches(host_ip_list, link_ip_list) or crawl_externally:
			print("Match")
			print("Crawling to: " + link)
			http_crawl(host, port, crawl_externally, this_level_links)
		else:
			print("No match")
	
	#Find any input forms
	#Make a dict of link, form boolean


	#for each link in links
		#sublist, forms = http_crawl

	#merge lists, forms

	#return list_of_links, forms

	return True


